{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hometown - Analytics Case\n",
    "## Pipeline Principal - Extra√ß√£o de Dados SIGEL/ANEEL\n",
    "\n",
    "Este notebook cont√©m o pipeline principal para extra√ß√£o de dados de aerogeradores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas\n",
    "\n",
    "# Adicionar src ao path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Imports do projeto\n",
    "from extraction.extractors import SigelExtractor\n",
    "from extraction.validators import (\n",
    "    validate_api_connection, \n",
    "    validate_response_structure,\n",
    "    validate_extraction_results\n",
    ")\n",
    "from config.settings import SIGEL_CONFIG\n",
    "from utils.logger import setup_logger\n",
    "from utils.exceptions import APIConnectionError, ValidationError\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(__name__, \"extraction.log\")\n",
    "\n",
    "print(\"Setup conclu√≠do!\")\n",
    "print(f\"Projeto root: {project_root}\")\n",
    "print(f\"URL da API: {SIGEL_CONFIG['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Valida√ß√£o da Conectividade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar conex√£o com a API\n",
    "try:\n",
    "    logger.info(\"Iniciando valida√ß√£o de conectividade...\")\n",
    "    \n",
    "    api_url = SIGEL_CONFIG[\"url\"]\n",
    "    connection_ok = validate_api_connection(api_url)\n",
    "    \n",
    "    if connection_ok:\n",
    "        print(\"‚úÖ API conectada com sucesso!\")\n",
    "        logger.info(\"Valida√ß√£o de conectividade conclu√≠da com sucesso\")\n",
    "    else:\n",
    "        print(\"‚ùå Falha na conex√£o com API\")\n",
    "        \n",
    "except (APIConnectionError, ValidationError) as e:\n",
    "    print(f\"‚ùå Erro de valida√ß√£o: {e}\")\n",
    "    logger.error(f\"Erro de valida√ß√£o: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro inesperado: {e}\")\n",
    "    logger.error(f\"Erro inesperado durante valida√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extra√ß√£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar extra√ß√£o completa\n",
    "try:\n",
    "    logger.info(\"Iniciando extra√ß√£o de dados...\")\n",
    "    print(\"üöÄ Iniciando extra√ß√£o de dados dos aerogeradores...\")\n",
    "    \n",
    "    # Instanciar extrator\n",
    "    extractor = SigelExtractor()\n",
    "    \n",
    "    # Executar extra√ß√£o\n",
    "    saved_files = extractor.extract_all_data()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extra√ß√£o conclu√≠da!\")\n",
    "    print(f\"üìÅ Arquivos salvos: {len(saved_files)}\")\n",
    "    \n",
    "    # Mostrar alguns arquivos salvos\n",
    "    print(\"\\nüìã Primeiros arquivos salvos:\")\n",
    "    for i, file_path in enumerate(saved_files[:5]):\n",
    "        print(f\"  {i+1}. {Path(file_path).name}\")\n",
    "    \n",
    "    if len(saved_files) > 5:\n",
    "        print(f\"  ... e mais {len(saved_files) - 5} arquivos\")\n",
    "        \n",
    "    logger.info(f\"Extra√ß√£o conclu√≠da com sucesso. {len(saved_files)} arquivos salvos\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro durante extra√ß√£o: {e}\")\n",
    "    logger.error(f\"Erro durante extra√ß√£o: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Valida√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar resultados da extra√ß√£o\n",
    "try:\n",
    "    if 'saved_files' in locals() and saved_files:\n",
    "        logger.info(\"Iniciando valida√ß√£o dos resultados...\")\n",
    "        print(\"üîç Validando resultados da extra√ß√£o...\")\n",
    "        \n",
    "        # Valida√ß√£o b√°sica (expandir posteriormente)\n",
    "        validation_ok = validate_extraction_results(\n",
    "            saved_files=saved_files,\n",
    "            expected_records=0  # TODO: usar contagem real\n",
    "        )\n",
    "        \n",
    "        if validation_ok:\n",
    "            print(\"‚úÖ Valida√ß√£o dos resultados conclu√≠da!\")\n",
    "            logger.info(\"Valida√ß√£o dos resultados conclu√≠da com sucesso\")\n",
    "        else:\n",
    "            print(\"‚ùå Falha na valida√ß√£o dos resultados\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nenhum arquivo para validar. Execute a extra√ß√£o primeiro.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro durante valida√ß√£o: {e}\")\n",
    "    logger.error(f\"Erro durante valida√ß√£o dos resultados: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resumo da Execu√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resumo final\n",
    "if 'saved_files' in locals():\n",
    "    print(\"üìä RESUMO DA EXTRA√á√ÉO\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"üóÇÔ∏è  Total de arquivos: {len(saved_files)}\")\n",
    "    \n",
    "    # Calcular tamanho total dos arquivos\n",
    "    total_size = 0\n",
    "    for file_path in saved_files:\n",
    "        if Path(file_path).exists():\n",
    "            total_size += Path(file_path).stat().st_size\n",
    "    \n",
    "    print(f\"üíæ Tamanho total: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"üìÅ Localiza√ß√£o: data/raw/\")\n",
    "    print(f\"üïí Padr√£o de nomes: aerogeradores_raw_YYYYMMDD_HHMMSS_page_XXXX.json\")\n",
    "    \n",
    "    # Status final\n",
    "    print(\"\\nüéâ Extra√ß√£o de dados conclu√≠da com sucesso!\")\n",
    "    print(\"üîÑ Pr√≥ximo passo: Transforma√ß√£o dos dados (JSON ‚Üí Parquet)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhuma extra√ß√£o foi executada nesta sess√£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Debug e Explora√ß√£o (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar estrutura de um arquivo extra√≠do\n",
    "import json\n",
    "\n",
    "if 'saved_files' in locals() and saved_files:\n",
    "    # Ler primeiro arquivo como exemplo\n",
    "    first_file = saved_files[0]\n",
    "    print(f\"üîç Explorando estrutura do arquivo: {Path(first_file).name}\")\n",
    "    \n",
    "    with open(first_file, 'r', encoding='utf-8') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nüìã Estrutura do JSON:\")\n",
    "    print(f\"  - Chaves principais: {list(sample_data.keys())}\")\n",
    "    \n",
    "    if 'features' in sample_data:\n",
    "        features = sample_data['features']\n",
    "        print(f\"  - Total de features: {len(features)}\")\n",
    "        \n",
    "        if features:\n",
    "            first_feature = features[0]\n",
    "            print(f\"  - Estrutura da feature:\")\n",
    "            print(f\"    - Chaves: {list(first_feature.keys())}\")\n",
    "            \n",
    "            if 'attributes' in first_feature:\n",
    "                attrs = first_feature['attributes']\n",
    "                print(f\"    - Attributes: {list(attrs.keys())[:10]}...\")  # Primeiros 10\n",
    "            \n",
    "            if 'geometry' in first_feature:\n",
    "                geom = first_feature['geometry']\n",
    "                print(f\"    - Geometry type: {geom.get('type', 'N/A')}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum arquivo dispon√≠vel para explora√ß√£o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de import\n",
    "try:\n",
    "    from transformation.processors import DataProcessor\n",
    "    from transformation.geo_utils import extract_coordinates, validate_geometry\n",
    "    print(\"‚úÖ Imports da transforma√ß√£o OK!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro no import: {e}\")\n",
    "\n",
    "# Teste b√°sico\n",
    "processor = DataProcessor()\n",
    "json_files = processor.discover_raw_files()\n",
    "print(f\"üìÅ Arquivos JSON encontrados: {len(json_files)}\")\n",
    "\n",
    "# Testar um arquivo s√≥\n",
    "if json_files:\n",
    "    print(f\"üîç Testando: {json_files[0].name}\")\n",
    "    test_result = processor.process_single_file(json_files[0])\n",
    "    print(f\"‚úÖ Resultado: {test_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar todos os 24 arquivos JSON ‚Üí Parquet\n",
    "print(\"üöÄ Processando todos os arquivos...\")\n",
    "all_results = processor.process_all_files()\n",
    "\n",
    "print(f\"\\nüìä RESULTADO FINAL:\")\n",
    "print(f\"‚úÖ Arquivos processados: {len(all_results)}\")\n",
    "\n",
    "# Verificar tamanho total\n",
    "total_size = 0\n",
    "for file_path in all_results:\n",
    "    if Path(file_path).exists():\n",
    "        total_size += Path(file_path).stat().st_size\n",
    "\n",
    "print(f\"üíæ Tamanho total Parquets: {total_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"üóÇÔ∏è Localiza√ß√£o: data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de import\n",
    "try:\n",
    "    from consolidation.consolidators import DataConsolidator\n",
    "    print(\"‚úÖ Import da consolida√ß√£o OK!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro no import: {e}\")\n",
    "\n",
    "# Instanciar consolidador\n",
    "consolidator = DataConsolidator()\n",
    "\n",
    "# Descobrir arquivos Parquet\n",
    "parquet_files = consolidator.discover_parquet_files()\n",
    "print(f\"üìÅ Arquivos Parquet encontrados: {len(parquet_files)}\")\n",
    "\n",
    "# Executar consolida√ß√£o completa\n",
    "print(\"üöÄ Iniciando consolida√ß√£o completa...\")\n",
    "output_path = consolidator.consolidate_all()\n",
    "\n",
    "print(f\"\\n‚úÖ Consolida√ß√£o conclu√≠da!\")\n",
    "print(f\"üìÑ Arquivo CSV: {Path(output_path).name}\")\n",
    "print(f\"üìÅ Localiza√ß√£o: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar import do pandas\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Carregar o CSV para an√°lise\n",
    "df_final = pd.read_csv(output_path)\n",
    "\n",
    "# Gerar resumo\n",
    "summary = consolidator.get_data_summary(df_final)\n",
    "\n",
    "print(\"\\nüìä RESUMO DOS DADOS CONSOLIDADOS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìù Total de registros: {summary['total_records']:,}\")\n",
    "print(f\"üìã Total de colunas: {summary['total_columns']}\")\n",
    "print(f\"üíæ Uso de mem√≥ria: {summary['memory_usage_mb']:.2f} MB\")\n",
    "\n",
    "if 'coordinate_stats' in summary:\n",
    "    coords = summary['coordinate_stats']\n",
    "    print(f\"\\nüó∫Ô∏è COORDENADAS:\")\n",
    "    print(f\"  Latitude: {coords['lat_min']:.6f} ‚Üí {coords['lat_max']:.6f}\")\n",
    "    print(f\"  Longitude: {coords['lon_min']:.6f} ‚Üí {coords['lon_max']:.6f}\")\n",
    "\n",
    "print(f\"\\nüìÇ Colunas dispon√≠veis:\")\n",
    "for i, col in enumerate(summary['columns'][:10], 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "if len(summary['columns']) > 10:\n",
    "    print(f\"  ... e mais {len(summary['columns']) - 10} colunas\")\n",
    "\n",
    "# Verificar tamanho do arquivo\n",
    "file_size = Path(output_path).stat().st_size / 1024 / 1024\n",
    "print(f\"\\nüíæ Tamanho do CSV: {file_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
