{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè† Hometown - Analytics Case\n",
    "## Pipeline Principal - Dados de Aerogeradores SIGEL/ANEEL\n",
    "\n",
    "Este notebook implementa um pipeline completo e idempotente para:\n",
    "- **Extra√ß√£o** de dados da API SIGEL/ANEEL\n",
    "- **Transforma√ß√£o** de JSON para Parquet otimizado\n",
    "- **Consolida√ß√£o** em CSV final para Tableau\n",
    "\n",
    "### Caracter√≠sticas do Pipeline:\n",
    "- ‚úÖ **Idempotente**: N√£o reprocessa dados desnecessariamente\n",
    "- ‚úÖ **Baseado em dados**: Usa `DATA_ATUALIZACAO` da pr√≥pria API\n",
    "- ‚úÖ **Paralelo**: ThreadPool para performance\n",
    "- ‚úÖ **Robusto**: Retry, valida√ß√µes e logs detalhados\n",
    "- ‚úÖ **Limpo**: Auto-limpeza de dados antigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üì¶ 1. Setup e Configura√ß√£o Inicial\n",
    "\n",
    "Configura√ß√£o do ambiente, imports e valida√ß√£o da conectividade com a API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configura√ß√£o do ambiente...\n",
      "üìÅ Projeto root: /home/victor-jose/Documents/projetos/hometown\n",
      "üìÅ Src path: /home/victor-jose/Documents/projetos/hometown/src\n",
      "‚úÖ Paths configurados!\n"
     ]
    }
   ],
   "source": [
    "# Imports b√°sicos e configura√ß√£o de paths\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar path do projeto\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(\"üîß Configura√ß√£o do ambiente...\")\n",
    "print(f\"üìÅ Projeto root: {project_root}\")\n",
    "print(f\"üìÅ Src path: {src_path}\")\n",
    "print(\"‚úÖ Paths configurados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö M√≥dulos importados com sucesso!\n",
      "üåê API URL: https://sigel.aneel.gov.br/arcgis/rest/services/PORTAL/WFS/MapServer/0/query\n",
      "üìÑ Tamanho da p√°gina: 1000 registros\n",
      "‚úÖ Setup conclu√≠do!\n"
     ]
    }
   ],
   "source": [
    "# Imports dos m√≥dulos do projeto\n",
    "from extraction.extractors import SigelExtractor\n",
    "from extraction.validators import validate_api_connection, validate_extraction_results\n",
    "from transformation.processors import DataProcessor\n",
    "from consolidation.consolidators import DataConsolidator\n",
    "from config.settings import SIGEL_CONFIG\n",
    "from utils.logger import setup_logger\n",
    "from utils.exceptions import APIConnectionError, ValidationError\n",
    "\n",
    "# Setup do logger principal\n",
    "logger = setup_logger(__name__, \"pipeline.log\")\n",
    "\n",
    "print(\"üìö M√≥dulos importados com sucesso!\")\n",
    "print(f\"üåê API URL: {SIGEL_CONFIG['url']}\")\n",
    "print(f\"üìÑ Tamanho da p√°gina: {SIGEL_CONFIG['page_size']} registros\")\n",
    "print(\"‚úÖ Setup conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Valida√ß√£o da Conectividade\n",
    "\n",
    "Testa a conex√£o com a API SIGEL/ANEEL antes de iniciar o pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Testando conectividade com a API SIGEL/ANEEL...\n",
      "2025-06-01 17:41:43 - extraction.validators - INFO - API conectada com sucesso. Total de registros: 23522\n",
      "‚úÖ API conectada com sucesso!\n",
      "2025-06-01 17:41:43 - __main__ - INFO - Valida√ß√£o de conectividade conclu√≠da com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Testar conectividade com a API\n",
    "try:\n",
    "    print(\"üåê Testando conectividade com a API SIGEL/ANEEL...\")\n",
    "    \n",
    "    api_url = SIGEL_CONFIG[\"url\"]\n",
    "    connection_ok = validate_api_connection(api_url)\n",
    "    \n",
    "    if connection_ok:\n",
    "        print(\"‚úÖ API conectada com sucesso!\")\n",
    "        logger.info(\"Valida√ß√£o de conectividade conclu√≠da com sucesso\")\n",
    "    else:\n",
    "        print(\"‚ùå Falha na conex√£o com API\")\n",
    "        \n",
    "except (APIConnectionError, ValidationError) as e:\n",
    "    print(f\"‚ùå Erro de valida√ß√£o: {e}\")\n",
    "    logger.error(f\"Erro de valida√ß√£o: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro inesperado: {e}\")\n",
    "    logger.error(f\"Erro inesperado durante valida√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üßπ 2. Comandos de Limpeza e Manuten√ß√£o\n",
    "\n",
    "Fun√ß√µes utilit√°rias para gerenciar dados e fazer limpeza quando necess√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä STATUS ATUAL DOS DADOS:\n",
      "========================================\n",
      "üóÇÔ∏è Arquivos JSON (raw): 0\n",
      "üì¶ Arquivos Parquet (processed): 0\n",
      "üìÑ Arquivos CSV (output): 0\n",
      "üíæ Tamanho total: 0.00 MB\n",
      "\n",
      "==================================================\n",
      "üîß COMANDOS DISPON√çVEIS:\n",
      "‚Ä¢ show_current_data_status() - Mostra status dos dados\n",
      "‚Ä¢ cleanup_all_data() - Remove todos os dados\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def show_current_data_status():\n",
    "    \"\"\"Exibe status atual dos dados no projeto\"\"\"\n",
    "    print(\"üìä STATUS ATUAL DOS DADOS:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Contar arquivos em cada etapa\n",
    "    raw_files = list(Path(\"data/raw\").glob(\"aerogeradores_raw_*.json\")) if Path(\"data/raw\").exists() else []\n",
    "    processed_files = list(Path(\"data/processed\").glob(\"aerogeradores_processed_*.parquet\")) if Path(\"data/processed\").exists() else []\n",
    "    output_files = list(Path(\"data/output\").glob(\"aerogeradores_consolidado_*.csv\")) if Path(\"data/output\").exists() else []\n",
    "    \n",
    "    print(f\"üóÇÔ∏è Arquivos JSON (raw): {len(raw_files)}\")\n",
    "    print(f\"üì¶ Arquivos Parquet (processed): {len(processed_files)}\")\n",
    "    print(f\"üìÑ Arquivos CSV (output): {len(output_files)}\")\n",
    "    \n",
    "    # Calcular tamanhos\n",
    "    total_size = 0\n",
    "    for files, label in [(raw_files, \"JSONs\"), (processed_files, \"Parquets\"), (output_files, \"CSVs\")]:\n",
    "        if files:\n",
    "            size = sum(f.stat().st_size for f in files) / 1024 / 1024\n",
    "            total_size += size\n",
    "            print(f\"üíæ Tamanho {label}: {size:.2f} MB\")\n",
    "    \n",
    "    print(f\"üíæ Tamanho total: {total_size:.2f} MB\")\n",
    "    \n",
    "    return {\n",
    "        'raw_count': len(raw_files),\n",
    "        'processed_count': len(processed_files),\n",
    "        'output_count': len(output_files),\n",
    "        'total_size_mb': total_size\n",
    "    }\n",
    "\n",
    "def cleanup_all_data():\n",
    "    \"\"\"Remove todos os dados para recome√ßar do zero\"\"\"\n",
    "    print(\"üßπ INICIANDO LIMPEZA COMPLETA...\")\n",
    "    \n",
    "    # Instanciar classes para usar m√©todos de limpeza\n",
    "    extractor = SigelExtractor()\n",
    "    processor = DataProcessor()\n",
    "    consolidator = DataConsolidator()\n",
    "    \n",
    "    # Executar limpeza de cada etapa\n",
    "    raw_removed = extractor.cleanup_all_raw_data()\n",
    "    processed_removed = processor.cleanup_all_processed_data()\n",
    "    output_removed = consolidator.cleanup_all_output_data()\n",
    "    \n",
    "    total_removed = raw_removed + processed_removed + output_removed\n",
    "    \n",
    "    print(f\"üìä RESUMO DA LIMPEZA:\")\n",
    "    print(f\"  üóÇÔ∏è JSONs removidos: {raw_removed}\")\n",
    "    print(f\"  üì¶ Parquets removidos: {processed_removed}\")\n",
    "    print(f\"  üìÑ CSVs removidos: {output_removed}\")\n",
    "    print(f\"  üóëÔ∏è Total de arquivos: {total_removed}\")\n",
    "    print(f\"‚úÖ Limpeza completa conclu√≠da!\")\n",
    "    \n",
    "    return total_removed\n",
    "\n",
    "# Mostrar status inicial\n",
    "status = show_current_data_status()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîß COMANDOS DISPON√çVEIS:\")\n",
    "print(\"‚Ä¢ show_current_data_status() - Mostra status dos dados\")\n",
    "print(\"‚Ä¢ cleanup_all_data() - Remove todos os dados\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ 3. Pipeline Principal - Extra√ß√£o de Dados\n",
    "\n",
    "Extra√ß√£o idempotente de dados da API SIGEL/ANEEL. O sistema verifica automaticamente se os dados mudaram na API antes de fazer nova extra√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFICANDO FRESHNESS DOS DADOS...\n",
      "==================================================\n",
      "2025-06-01 17:41:53 - extraction.extractors - INFO - Verificando freshness dos dados...\n",
      "2025-06-01 17:41:54 - extraction.extractors - INFO - API √∫ltima atualiza√ß√£o: 1673359590000\n",
      "2025-06-01 17:41:54 - extraction.extractors - INFO - Nossa √∫ltima extra√ß√£o: 0\n",
      "2025-06-01 17:41:54 - extraction.extractors - INFO - Precisa atualizar: True\n",
      "üìä STATUS DOS DADOS:\n",
      "  üåê API √∫ltima atualiza√ß√£o: 1673359590000\n",
      "  üíæ Nossa √∫ltima extra√ß√£o: 0\n",
      "  üîÑ Precisa atualizar: True\n",
      "  üìÖ Primeira execu√ß√£o - nenhuma extra√ß√£o anterior\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Instanciar extrator e verificar freshness dos dados\n",
    "print(\"üîç VERIFICANDO FRESHNESS DOS DADOS...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "extractor = SigelExtractor()\n",
    "freshness = extractor.check_data_freshness()\n",
    "\n",
    "print(f\"üìä STATUS DOS DADOS:\")\n",
    "print(f\"  üåê API √∫ltima atualiza√ß√£o: {freshness['api_latest_update']}\")\n",
    "print(f\"  üíæ Nossa √∫ltima extra√ß√£o: {freshness['our_last_extraction']}\")\n",
    "print(f\"  üîÑ Precisa atualizar: {freshness['needs_refresh']}\")\n",
    "\n",
    "if freshness['last_extraction_time']:\n",
    "    print(f\"  üìÖ √öltima extra√ß√£o em: {freshness['last_extraction_time']}\")\n",
    "    print(f\"  üìä Registros da √∫ltima extra√ß√£o: {freshness['last_total_records']:,}\")\n",
    "else:\n",
    "    print(f\"  üìÖ Primeira execu√ß√£o - nenhuma extra√ß√£o anterior\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ EXECUTANDO EXTRA√á√ÉO DE DADOS...\n",
      "(O sistema decide automaticamente se precisa extrair baseado na DATA_ATUALIZACAO da API)\n",
      "\n",
      "2025-06-01 17:41:56 - extraction.extractors - INFO - Verificando freshness dos dados...\n",
      "2025-06-01 17:41:56 - extraction.extractors - INFO - API √∫ltima atualiza√ß√£o: 1673359590000\n",
      "2025-06-01 17:41:56 - extraction.extractors - INFO - Nossa √∫ltima extra√ß√£o: 0\n",
      "2025-06-01 17:41:56 - extraction.extractors - INFO - Precisa atualizar: True\n",
      "2025-06-01 17:41:56 - extraction.extractors - INFO - üîÑ Dados da API foram atualizados, executando nova extra√ß√£o...\n",
      "2025-06-01 17:41:57 - extraction.extractors - INFO - üìä Total de registros: 23522\n",
      "2025-06-01 17:41:57 - extraction.extractors - INFO - üìÑ Total de p√°ginas: 24\n",
      "2025-06-01 17:41:57 - extraction.extractors - INFO - üìÖ Data de atualiza√ß√£o da API: 1673359590000\n",
      "2025-06-01 17:41:57 - extraction.extractors - INFO - Extraindo p√°gina 1/24\n",
      "2025-06-01 17:41:57 - extraction.extractors - INFO - Extraindo p√°gina 2/24\n",
      "2025-06-01 17:41:57 - extraction.extractors - INFO - Extraindo p√°gina 3/24\n",
      "2025-06-01 17:41:58 - extraction.extractors - INFO - Extraindo p√°gina 4/24\n",
      "2025-06-01 17:41:58 - extraction.extractors - INFO - Extraindo p√°gina 5/24\n",
      "2025-06-01 17:41:59 - extraction.extractors - INFO - Extraindo p√°gina 6/24\n",
      "2025-06-01 17:41:59 - extraction.extractors - INFO - Extraindo p√°gina 7/24\n",
      "2025-06-01 17:41:59 - extraction.extractors - INFO - Extraindo p√°gina 8/24\n",
      "2025-06-01 17:42:00 - extraction.extractors - INFO - Extraindo p√°gina 9/24\n",
      "2025-06-01 17:42:00 - extraction.extractors - INFO - Extraindo p√°gina 10/24\n",
      "2025-06-01 17:42:00 - extraction.extractors - INFO - Extraindo p√°gina 11/24\n",
      "2025-06-01 17:42:01 - extraction.extractors - INFO - Extraindo p√°gina 12/24\n",
      "2025-06-01 17:42:01 - extraction.extractors - INFO - Extraindo p√°gina 13/24\n",
      "2025-06-01 17:42:01 - extraction.extractors - INFO - Extraindo p√°gina 14/24\n",
      "2025-06-01 17:42:02 - extraction.extractors - INFO - Extraindo p√°gina 15/24\n",
      "2025-06-01 17:42:02 - extraction.extractors - INFO - Extraindo p√°gina 16/24\n",
      "2025-06-01 17:42:02 - extraction.extractors - INFO - Extraindo p√°gina 17/24\n",
      "2025-06-01 17:42:03 - extraction.extractors - INFO - Extraindo p√°gina 18/24\n",
      "2025-06-01 17:42:03 - extraction.extractors - INFO - Extraindo p√°gina 19/24\n",
      "2025-06-01 17:42:03 - extraction.extractors - INFO - Extraindo p√°gina 20/24\n",
      "2025-06-01 17:42:04 - extraction.extractors - INFO - Extraindo p√°gina 21/24\n",
      "2025-06-01 17:42:04 - extraction.extractors - INFO - Extraindo p√°gina 22/24\n",
      "2025-06-01 17:42:04 - extraction.extractors - INFO - Extraindo p√°gina 23/24\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - Extraindo p√°gina 24/24\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 4 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 13 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 11 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 23 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 1 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 9 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 8 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 21 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 15 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 16 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 3 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 18 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 5 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 7 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 6 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 17 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 12 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 22 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 2 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 10 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 14 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 19 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 20 salva: 1000 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - P√°gina 24 salva: 522 registros\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - Metadata salvo: √∫ltima atualiza√ß√£o 1673359590000\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - ‚úÖ Extra√ß√£o conclu√≠da!\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - üìÅ Arquivos salvos: 24\n",
      "2025-06-01 17:42:05 - extraction.extractors - INFO - üìä Registros processados: 23522/23522\n",
      "\n",
      "üìä RESULTADO DA EXTRA√á√ÉO:\n",
      "‚úÖ Arquivos JSON salvos: 24\n",
      "\n",
      "üìã Arquivos salvos (primeiros 3):\n",
      "  1. aerogeradores_raw_20250601_174156_page_0004.json (731.2 KB)\n",
      "  2. aerogeradores_raw_20250601_174156_page_0013.json (743.6 KB)\n",
      "  3. aerogeradores_raw_20250601_174156_page_0011.json (749.3 KB)\n",
      "  ... e mais 21 arquivos\n",
      "\n",
      "üíæ Tamanho total: 16.94 MB\n",
      "üìÅ Localiza√ß√£o: data/raw/\n",
      "2025-06-01 17:42:05 - __main__ - INFO - Extra√ß√£o conclu√≠da: 24 arquivos\n"
     ]
    }
   ],
   "source": [
    "# Executar extra√ß√£o (idempotente - s√≥ extrai se necess√°rio)\n",
    "print(\"üöÄ EXECUTANDO EXTRA√á√ÉO DE DADOS...\")\n",
    "print(\"(O sistema decide automaticamente se precisa extrair baseado na DATA_ATUALIZACAO da API)\\n\")\n",
    "\n",
    "try:\n",
    "    # Extra√ß√£o idempotente (force_refresh=False por padr√£o)\n",
    "    saved_files = extractor.extract_all_data()\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADO DA EXTRA√á√ÉO:\")\n",
    "    print(f\"‚úÖ Arquivos JSON salvos: {len(saved_files)}\")\n",
    "    \n",
    "    if saved_files:\n",
    "        # Mostrar alguns arquivos como exemplo\n",
    "        print(f\"\\nüìã Arquivos salvos (primeiros 3):\")\n",
    "        for i, file_path in enumerate(saved_files[:3]):\n",
    "            filename = Path(file_path).name\n",
    "            size = Path(file_path).stat().st_size / 1024  # KB\n",
    "            print(f\"  {i+1}. {filename} ({size:.1f} KB)\")\n",
    "        \n",
    "        if len(saved_files) > 3:\n",
    "            print(f\"  ... e mais {len(saved_files) - 3} arquivos\")\n",
    "        \n",
    "        # Calcular tamanho total\n",
    "        total_size = sum(Path(f).stat().st_size for f in saved_files) / 1024 / 1024\n",
    "        print(f\"\\nüíæ Tamanho total: {total_size:.2f} MB\")\n",
    "        print(f\"üìÅ Localiza√ß√£o: data/raw/\")\n",
    "    \n",
    "    logger.info(f\"Extra√ß√£o conclu√≠da: {len(saved_files)} arquivos\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro durante extra√ß√£o: {e}\")\n",
    "    logger.error(f\"Erro durante extra√ß√£o: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VALIDANDO DADOS EXTRA√çDOS...\n",
      "2025-06-01 17:43:23 - extraction.validators - INFO - Valida√ß√£o da extra√ß√£o conclu√≠da. 24 arquivos salvos\n",
      "‚úÖ Valida√ß√£o dos dados extra√≠dos conclu√≠da!\n",
      "\n",
      "üîç EXPLORANDO ESTRUTURA DOS DADOS:\n",
      "üìÅ Arquivo exemplo: aerogeradores_raw_20250601_174156_page_0004.json\n",
      "üìã Chaves principais: ['displayFieldName', 'fieldAliases', 'geometryType', 'spatialReference', 'fields', 'features', 'exceededTransferLimit']\n",
      "üìä Features neste arquivo: 1000\n",
      "üè∑Ô∏è Campos dispon√≠veis: ['POT_MW', 'ALT_TOTAL', 'ALT_TORRE', 'DIAM_ROTOR', 'DATA_ATUALIZACAO', 'EOL_VERSAO_ID', 'NOME_EOL', 'DEN_AEG']...\n",
      "üìÖ DATA_ATUALIZACAO exemplo: 1666625847000\n",
      "2025-06-01 17:43:23 - __main__ - INFO - Valida√ß√£o dos dados extra√≠dos conclu√≠da com sucesso\n"
     ]
    }
   ],
   "source": [
    "# Valida√ß√£o dos dados extra√≠dos\n",
    "if 'saved_files' in locals() and saved_files:\n",
    "    print(\"üîç VALIDANDO DADOS EXTRA√çDOS...\")\n",
    "    \n",
    "    try:\n",
    "        # Valida√ß√£o usando fun√ß√£o do m√≥dulo validators\n",
    "        validation_ok = validate_extraction_results(\n",
    "            saved_files=saved_files,\n",
    "            expected_records=0  # A fun√ß√£o vai descobrir automaticamente\n",
    "        )\n",
    "        \n",
    "        if validation_ok:\n",
    "            print(\"‚úÖ Valida√ß√£o dos dados extra√≠dos conclu√≠da!\")\n",
    "        else:\n",
    "            print(\"‚ùå Falha na valida√ß√£o dos dados extra√≠dos\")\n",
    "            \n",
    "        # Explorar estrutura de um arquivo como exemplo\n",
    "        print(\"\\nüîç EXPLORANDO ESTRUTURA DOS DADOS:\")\n",
    "        first_file = saved_files[0]\n",
    "        print(f\"üìÅ Arquivo exemplo: {Path(first_file).name}\")\n",
    "        \n",
    "        with open(first_file, 'r', encoding='utf-8') as f:\n",
    "            sample_data = json.load(f)\n",
    "        \n",
    "        print(f\"üìã Chaves principais: {list(sample_data.keys())}\")\n",
    "        \n",
    "        if 'features' in sample_data:\n",
    "            features = sample_data['features']\n",
    "            print(f\"üìä Features neste arquivo: {len(features)}\")\n",
    "            \n",
    "            if features:\n",
    "                first_feature = features[0]\n",
    "                attrs = first_feature.get('attributes', {})\n",
    "                print(f\"üè∑Ô∏è Campos dispon√≠veis: {list(attrs.keys())[:8]}...\")  # Primeiros 8\n",
    "                \n",
    "                # Mostrar campo de data de atualiza√ß√£o\n",
    "                if 'DATA_ATUALIZACAO' in attrs:\n",
    "                    data_atualizacao = attrs['DATA_ATUALIZACAO']\n",
    "                    print(f\"üìÖ DATA_ATUALIZACAO exemplo: {data_atualizacao}\")\n",
    "        \n",
    "        logger.info(\"Valida√ß√£o dos dados extra√≠dos conclu√≠da com sucesso\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro durante valida√ß√£o: {e}\")\n",
    "        logger.error(f\"Erro durante valida√ß√£o: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum arquivo para validar. A extra√ß√£o pode ter sido pulada (idempot√™ncia).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ 4. Pipeline de Transforma√ß√£o - JSON ‚Üí Parquet\n",
    "\n",
    "Transforma os arquivos JSON em Parquet otimizado com dados geogr√°ficos processados (lat/long extra√≠dos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ VERIFICANDO NECESSIDADE DE TRANSFORMA√á√ÉO...\n",
      "==================================================\n",
      "2025-06-01 17:43:27 - transformation.processors - INFO - Descobertos 24 arquivos JSON para processar\n",
      "üìä STATUS DA TRANSFORMA√á√ÉO:\n",
      "  üîÑ Precisa transformar: False\n",
      "  üóÇÔ∏è Arquivos JSON encontrados: 24\n",
      "  üì¶ Arquivos Parquet existentes: 24\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Instanciar processador e verificar necessidade de transforma√ß√£o\n",
    "print(\"üîÑ VERIFICANDO NECESSIDADE DE TRANSFORMA√á√ÉO...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "processor = DataProcessor()\n",
    "transform_check = processor.check_transformation_needed()\n",
    "\n",
    "print(f\"üìä STATUS DA TRANSFORMA√á√ÉO:\")\n",
    "print(f\"  üîÑ Precisa transformar: {transform_check['needs_transformation']}\")\n",
    "\n",
    "\n",
    "if 'json_count' in transform_check:\n",
    "    print(f\"  üóÇÔ∏è Arquivos JSON encontrados: {transform_check['json_count']}\")\n",
    "if 'parquet_count' in transform_check:\n",
    "    print(f\"  üì¶ Arquivos Parquet existentes: {transform_check['parquet_count']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ EXECUTANDO TRANSFORMA√á√ÉO JSON ‚Üí PARQUET...\n",
      "(Sistema converte JSON para Parquet com coordenadas lat/long extra√≠das)\n",
      "\n",
      "2025-06-01 17:43:30 - transformation.processors - INFO - Descobertos 24 arquivos JSON para processar\n",
      "‚ùå Erro durante transforma√ß√£o: 'reason'\n",
      "2025-06-01 17:43:30 - __main__ - ERROR - Erro durante transforma√ß√£o: 'reason'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'reason'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m(Sistema converte JSON para Parquet com coordenadas lat/long extra√≠das)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Transforma√ß√£o idempotente (force_refresh=False por padr√£o)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     processed_files = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_all_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä RESULTADO DA TRANSFORMA√á√ÉO:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Arquivos Parquet criados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projetos/hometown/src/transformation/processors.py:161\u001b[39m, in \u001b[36mDataProcessor.process_all_files\u001b[39m\u001b[34m(self, max_workers, force_refresh)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_result[\u001b[33m'\u001b[39m\u001b[33mneeds_transformation\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    160\u001b[39m     existing_parquets = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.processed_data_path.glob(\u001b[33m\"\u001b[39m\u001b[33maerogeradores_processed_*.parquet\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcheck_result\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreason\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mstr\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m existing_parquets]\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyError\u001b[39m: 'reason'"
     ]
    }
   ],
   "source": [
    "# Executar transforma√ß√£o (idempotente - s√≥ processa se necess√°rio)\n",
    "print(\"üîÑ EXECUTANDO TRANSFORMA√á√ÉO JSON ‚Üí PARQUET...\")\n",
    "print(\"(Sistema converte JSON para Parquet com coordenadas lat/long extra√≠das)\\n\")\n",
    "\n",
    "try:\n",
    "    # Transforma√ß√£o idempotente (force_refresh=False por padr√£o)\n",
    "    processed_files = processor.process_all_files()\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADO DA TRANSFORMA√á√ÉO:\")\n",
    "    print(f\"‚úÖ Arquivos Parquet criados: {len(processed_files)}\")\n",
    "    \n",
    "    if processed_files:\n",
    "        # Calcular estat√≠sticas dos parquets\n",
    "        total_size = 0\n",
    "        total_records = 0\n",
    "        \n",
    "        print(f\"\\nüìã Arquivos Parquet (primeiros 3):\")\n",
    "        for i, file_path in enumerate(processed_files[:3]):\n",
    "            filename = Path(file_path).name\n",
    "            size = Path(file_path).stat().st_size / 1024  # KB\n",
    "            total_size += Path(file_path).stat().st_size\n",
    "            \n",
    "            # Contar registros no parquet\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                records = len(df)\n",
    "                total_records += records\n",
    "                print(f\"  {i+1}. {filename} ({size:.1f} KB, {records:,} registros)\")\n",
    "            except:\n",
    "                print(f\"  {i+1}. {filename} ({size:.1f} KB)\")\n",
    "        \n",
    "        if len(processed_files) > 3:\n",
    "            print(f\"  ... e mais {len(processed_files) - 3} arquivos\")\n",
    "            \n",
    "            # Contar registros restantes\n",
    "            for file_path in processed_files[3:]:\n",
    "                total_size += Path(file_path).stat().st_size\n",
    "                try:\n",
    "                    df = pd.read_parquet(file_path)\n",
    "                    total_records += len(df)\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        print(f\"\\nüíæ Tamanho total: {total_size / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"üìä Total de registros: {total_records:,}\")\n",
    "        print(f\"üìÅ Localiza√ß√£o: data/processed/\")\n",
    "        print(f\"üóúÔ∏è Compress√£o: {((total_size / 1024 / 1024) / 17) * 100:.1f}% do tamanho original JSON\")\n",
    "    \n",
    "    logger.info(f\"Transforma√ß√£o conclu√≠da: {len(processed_files)} parquets\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro durante transforma√ß√£o: {e}\")\n",
    "    logger.error(f\"Erro durante transforma√ß√£o: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä 5. Pipeline de Consolida√ß√£o - Parquet ‚Üí CSV Final\n",
    "\n",
    "Consolida todos os Parquets em um √∫nico CSV otimizado para o Tableau, com valida√ß√µes inteligentes de conte√∫do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar consolidador e verificar necessidade de consolida√ß√£o\n",
    "print(\"üìä VERIFICANDO NECESSIDADE DE CONSOLIDA√á√ÉO...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "consolidator = DataConsolidator()\n",
    "consolidation_check = consolidator.check_consolidation_needed()\n",
    "\n",
    "print(f\"üìä STATUS DA CONSOLIDA√á√ÉO:\")\n",
    "print(f\"  üîÑ Precisa consolidar: {consolidation_check['needs_consolidation']}\")\n",
    "print(f\"  üìù Motivo: {consolidation_check['reason']}\")\n",
    "\n",
    "if 'csv_records' in consolidation_check:\n",
    "    print(f\"  üìÑ Registros no CSV atual: {consolidation_check['csv_records']:,}\")\n",
    "if 'expected_records' in consolidation_check:\n",
    "    print(f\"  üì¶ Registros esperados: {consolidation_check['expected_records']:,}\")\n",
    "if 'parquet_count' in consolidation_check:\n",
    "    print(f\"  üì¶ Arquivos Parquet encontrados: {consolidation_check['parquet_count']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar consolida√ß√£o (idempotente - s√≥ consolida se necess√°rio)\n",
    "print(\"üìä EXECUTANDO CONSOLIDA√á√ÉO PARQUET ‚Üí CSV...\")\n",
    "print(\"(Sistema cria CSV final otimizado para Tableau com dados limpos e validados)\\n\")\n",
    "\n",
    "try:\n",
    "    # Consolida√ß√£o idempotente (force_refresh=False por padr√£o)\n",
    "    output_path = consolidator.consolidate_all()\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADO DA CONSOLIDA√á√ÉO:\")\n",
    "    \n",
    "    if output_path and Path(output_path).exists():\n",
    "        filename = Path(output_path).name\n",
    "        file_size = Path(output_path).stat().st_size / 1024 / 1024  # MB\n",
    "        \n",
    "        print(f\"‚úÖ CSV final criado: {filename}\")\n",
    "        print(f\"üíæ Tamanho do arquivo: {file_size:.2f} MB\")\n",
    "        print(f\"üìÅ Localiza√ß√£o completa: {output_path}\")\n",
    "        \n",
    "        # Analisar conte√∫do do CSV final\n",
    "        print(f\"\\nüîç AN√ÅLISE DO CSV FINAL:\")\n",
    "        df_final = pd.read_csv(output_path)\n",
    "        summary = consolidator.get_data_summary(df_final)\n",
    "        \n",
    "        print(f\"üìä Total de registros: {summary['total_records']:,}\")\n",
    "        print(f\"üìã Total de colunas: {summary['total_columns']}\")\n",
    "        print(f\"üíæ Uso de mem√≥ria: {summary['memory_usage_mb']:.2f} MB\")\n",
    "        \n",
    "        # Coordenadas geogr√°ficas\n",
    "        if 'coordinate_stats' in summary:\n",
    "            coords = summary['coordinate_stats']\n",
    "            print(f\"\\nüó∫Ô∏è COORDENADAS GEOGR√ÅFICAS:\")\n",
    "            print(f\"  üìç Latitude: {coords['lat_min']:.6f} ‚Üí {coords['lat_max']:.6f}\")\n",
    "            print(f\"  üìç Longitude: {coords['lon_min']:.6f} ‚Üí {coords['lon_max']:.6f}\")\n",
    "            print(f\"  üáßüá∑ Cobertura: Todo o territ√≥rio brasileiro\")\n",
    "        \n",
    "        # Estrutura das colunas\n",
    "        print(f\"\\nüìÇ ESTRUTURA DOS DADOS (primeiras 10 colunas):\")\n",
    "        for i, col in enumerate(summary['columns'][:10], 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        if len(summary['columns']) > 10:\n",
    "            print(f\"  ... e mais {len(summary['columns']) - 10} colunas\")\n",
    "        \n",
    "        print(f\"\\nüéØ CSV PRONTO PARA TABLEAU!\")\n",
    "        print(f\"   ‚Ä¢ Coordenadas no in√≠cio para mapas autom√°ticos\")\n",
    "        print(f\"   ‚Ä¢ Dados limpos e validados\")\n",
    "        print(f\"   ‚Ä¢ Formato otimizado para an√°lise\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Nenhum CSV foi criado (possivelmente devido √† idempot√™ncia)\")\n",
    "    \n",
    "    logger.info(f\"Consolida√ß√£o conclu√≠da: {filename if output_path else 'pulada'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro durante consolida√ß√£o: {e}\")\n",
    "    logger.error(f\"Erro durante consolida√ß√£o: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ 6. Testes de Idempot√™ncia\n",
    "\n",
    "Valida√ß√£o do comportamento idempotente do pipeline - execu√ß√µes subsequentes devem pular etapas desnecess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Teste completo de idempot√™ncia do pipeline\n",
    "print(\"üß™ TESTE DE IDEMPOT√äNCIA COMPLETO\")\n",
    "print(\"=\"*60)\n",
    "print(\"Executando pipeline novamente - deve pular todas as etapas se dados n√£o mudaram\\n\")\n",
    "\n",
    "# Reinstanciar todas as classes para teste limpo\n",
    "test_extractor = SigelExtractor()\n",
    "test_processor = DataProcessor()\n",
    "test_consolidator = DataConsolidator()\n",
    "\n",
    "print(\"üîç 1. TESTE DE EXTRA√á√ÉO:\")\n",
    "start_time = time.time()\n",
    "test_files = test_extractor.extract_all_data()  # Deve pular se dados iguais\n",
    "extraction_time = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Tempo: {extraction_time:.2f}s\")\n",
    "print(f\"   üìÅ Arquivos: {len(test_files)}\")\n",
    "\n",
    "print(\"\\nüîÑ 2. TESTE DE TRANSFORMA√á√ÉO:\")\n",
    "start_time = time.time()\n",
    "test_parquets = test_processor.process_all_files()  # Deve pular se parquets atualizados\n",
    "transform_time = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Tempo: {transform_time:.2f}s\")\n",
    "print(f\"   üì¶ Parquets: {len(test_parquets)}\")\n",
    "\n",
    "print(\"\\nüìä 3. TESTE DE CONSOLIDA√á√ÉO:\")\n",
    "start_time = time.time()\n",
    "test_csv = test_consolidator.consolidate_all()  # Deve pular se CSV atualizado\n",
    "consolidation_time = time.time() - start_time\n",
    "print(f\"   ‚è±Ô∏è Tempo: {consolidation_time:.2f}s\")\n",
    "print(f\"   üìÑ CSV: {Path(test_csv).name if test_csv else 'Nenhum'}\")\n",
    "\n",
    "total_time = extraction_time + transform_time + consolidation_time\n",
    "print(f\"\\n‚è±Ô∏è TEMPO TOTAL: {total_time:.2f}s\")\n",
    "print(f\"‚úÖ IDEMPOT√äNCIA {'FUNCIONANDO' if total_time < 5 else 'PODE TER PROBLEMAS'}!\")\n",
    "if total_time < 5:\n",
    "    print(\"   Pipeline executou rapidamente = dados n√£o foram reprocessados desnecessariamente\")\n",
    "else:\n",
    "    print(\"   Pipeline demorou = pode estar reprocessando dados desnecessariamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de force refresh - deve reprocessar tudo\n",
    "print(\"\\nüîÑ TESTE DE FORCE REFRESH\")\n",
    "print(\"=\"*40)\n",
    "print(\"‚ö†Ô∏è ATEN√á√ÉO: Isso vai reprocessar todos os dados (pode demorar alguns minutos)\")\n",
    "print(\"Descomente as linhas abaixo se quiser testar force refresh:\\n\")\n",
    "\n",
    "print(\"# Force refresh - descomente para executar:\")\n",
    "print(\"# force_files = test_extractor.extract_all_data(force_refresh=True)\")\n",
    "print(\"# force_parquets = test_processor.process_all_files(force_refresh=True)\")\n",
    "print(\"# force_csv = test_consolidator.consolidate_all(force_refresh=True)\")\n",
    "print(\"# print(f'Force refresh: {len(force_files)} JSONs, {len(force_parquets)} Parquets, 1 CSV')\")\n",
    "\n",
    "print(\"\\nüí° O force refresh deve:\")\n",
    "print(\"   ‚Ä¢ Limpar dados antigos automaticamente\")\n",
    "print(\"   ‚Ä¢ Reprocessar todos os dados do zero\")\n",
    "print(\"   ‚Ä¢ Gerar novos timestamps em todos os arquivos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üõ†Ô∏è 7. Comandos de Manuten√ß√£o Avan√ßada\n",
    "\n",
    "Ferramentas para limpeza, debug e manuten√ß√£o do pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comandos de limpeza e manuten√ß√£o\n",
    "print(\"üõ†Ô∏è COMANDOS DE MANUTEN√á√ÉO DISPON√çVEIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üìä STATUS E INFORMA√á√ïES:\")\n",
    "print(\"‚Ä¢ show_current_data_status() - Mostra status completo dos dados\")\n",
    "print(\"‚Ä¢ extractor.check_data_freshness() - Verifica se API mudou\")\n",
    "print(\"‚Ä¢ processor.check_transformation_needed() - Verifica necessidade de transforma√ß√£o\")\n",
    "print(\"‚Ä¢ consolidator.check_consolidation_needed() - Verifica necessidade de consolida√ß√£o\")\n",
    "\n",
    "print(\"\\nüßπ LIMPEZA:\")\n",
    "print(\"‚Ä¢ cleanup_all_data() - Remove TODOS os dados (reset completo)\")\n",
    "print(\"‚Ä¢ extractor.cleanup_all_raw_data() - Remove apenas JSONs\")\n",
    "print(\"‚Ä¢ processor.cleanup_all_processed_data() - Remove apenas Parquets\")\n",
    "print(\"‚Ä¢ consolidator.cleanup_all_output_data() - Remove apenas CSVs\")\n",
    "\n",
    "print(\"\\nüîÑ FORCE REFRESH:\")\n",
    "print(\"‚Ä¢ extractor.extract_all_data(force_refresh=True) - For√ßa nova extra√ß√£o\")\n",
    "print(\"‚Ä¢ processor.process_all_files(force_refresh=True) - For√ßa nova transforma√ß√£o\")\n",
    "print(\"‚Ä¢ consolidator.consolidate_all(force_refresh=True) - For√ßa nova consolida√ß√£o\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üí° DICAS:\")\n",
    "print(\"‚Ä¢ Use cleanup_all_data() se quiser recome√ßar do zero\")\n",
    "print(\"‚Ä¢ Use force_refresh=True se suspeitar de dados corrompidos\")\n",
    "print(\"‚Ä¢ O pipeline normal (sem par√¢metros) √© idempotente e eficiente\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà 8. Resumo Final e Pr√≥ximos Passos\n",
    "\n",
    "Status final do pipeline e orienta√ß√µes para usar os dados no Tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final completo do pipeline\n",
    "import time\n",
    "\n",
    "status = show_current_data_status()\n",
    "\n",
    "print(\"üìà RESUMO FINAL DO PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Status atual dos dados\n",
    "final_status = show_current_data_status()\n",
    "\n",
    "print(f\"\\nüéØ PIPELINE STATUS:\")\n",
    "if final_status['raw_count'] > 0:\n",
    "    print(f\"‚úÖ Extra√ß√£o: {final_status['raw_count']} arquivos JSON\")\n",
    "else:\n",
    "    print(f\"‚ùå Extra√ß√£o: Nenhum arquivo encontrado\")\n",
    "\n",
    "if final_status['processed_count'] > 0:\n",
    "    print(f\"‚úÖ Transforma√ß√£o: {final_status['processed_count']} arquivos Parquet\")\n",
    "else:\n",
    "    print(f\"‚ùå Transforma√ß√£o: Nenhum arquivo encontrado\")\n",
    "\n",
    "if final_status['output_count'] > 0:\n",
    "    print(f\"‚úÖ Consolida√ß√£o: {final_status['output_count']} arquivo(s) CSV\")\n",
    "    \n",
    "    # Encontrar CSV mais recente\n",
    "    csv_files = list(Path(\"data/output\").glob(\"aerogeradores_consolidado_*.csv\"))\n",
    "    if csv_files:\n",
    "        latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime)\n",
    "        csv_size = latest_csv.stat().st_size / 1024 / 1024\n",
    "        \n",
    "        # Contar registros\n",
    "        try:\n",
    "            df = pd.read_csv(latest_csv)\n",
    "            record_count = len(df)\n",
    "            print(f\"   üìÑ Arquivo: {latest_csv.name}\")\n",
    "            print(f\"   üìä Registros: {record_count:,} aerogeradores\")\n",
    "            print(f\"   üíæ Tamanho: {csv_size:.2f} MB\")\n",
    "            print(f\"   üìÅ Local: {latest_csv}\")\n",
    "        except:\n",
    "            print(f\"   üìÑ Arquivo: {latest_csv.name} ({csv_size:.2f} MB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Consolida√ß√£o: Nenhum arquivo encontrado\")\n",
    "\n",
    "print(f\"\\nüíæ Espa√ßo total usado: {final_status['total_size_mb']:.2f} MB\")\n",
    "\n",
    "# Verificar se pipeline est√° completo\n",
    "pipeline_complete = (final_status['raw_count'] > 0 and \n",
    "                    final_status['processed_count'] > 0 and \n",
    "                    final_status['output_count'] > 0)\n",
    "\n",
    "if pipeline_complete:\n",
    "    print(f\"\\nüéâ PIPELINE COMPLETO E FUNCIONAL!\")\n",
    "    print(f\"\\nüîÑ CARACTER√çSTICAS IMPLEMENTADAS:\")\n",
    "    print(f\"   ‚úÖ Idempotente - n√£o reprocessa dados desnecessariamente\")\n",
    "    print(f\"   ‚úÖ Baseado em DATA_ATUALIZACAO da API\")\n",
    "    print(f\"   ‚úÖ Processamento paralelo com ThreadPool\")\n",
    "    print(f\"   ‚úÖ Retry autom√°tico com backoff exponencial\")\n",
    "    print(f\"   ‚úÖ Logs detalhados de todas as opera√ß√µes\")\n",
    "    print(f\"   ‚úÖ Valida√ß√µes geogr√°ficas autom√°ticas\")\n",
    "    print(f\"   ‚úÖ Auto-limpeza de dados antigos\")\n",
    "    print(f\"   ‚úÖ Otimizado para Tableau (coordenadas primeiro)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è PIPELINE INCOMPLETO\")\n",
    "    print(f\"Execute as c√©lulas anteriores para completar o pipeline.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instru√ß√µes para uso no Tableau\n",
    "print(\"üìä PR√ìXIMOS PASSOS - TABLEAU DASHBOARD\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if final_status['output_count'] > 0:\n",
    "    csv_files = list(Path(\"data/output\").glob(\"aerogeradores_consolidado_*.csv\"))\n",
    "    if csv_files:\n",
    "        latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime)\n",
    "        \n",
    "        print(f\"üìÅ ARQUIVO PARA TABLEAU:\")\n",
    "        print(f\"   {latest_csv}\")\n",
    "        \n",
    "        print(f\"\\nüîß COMO USAR NO TABLEAU:\")\n",
    "        print(f\"   1. Abra o Tableau Public/Desktop\")\n",
    "        print(f\"   2. Conecte ao arquivo CSV acima\")\n",
    "        print(f\"   3. Latitude/Longitude j√° est√£o nas primeiras colunas\")\n",
    "        print(f\"   4. Tableau deve detectar automaticamente os campos geogr√°ficos\")\n",
    "        \n",
    "        print(f\"\\nüìà VISUALIZA√á√ïES SUGERIDAS:\")\n",
    "        print(f\"   ‚Ä¢ Mapa de pontos dos aerogeradores (latitude/longitude)\")\n",
    "        print(f\"   ‚Ä¢ Scatter plot: POT_MW vs ALT_TOTAL\")\n",
    "        print(f\"   ‚Ä¢ Box plot: distribui√ß√£o de pot√™ncias por regi√£o\")\n",
    "        print(f\"   ‚Ä¢ S√©rie temporal: usando DATA_ATUALIZACAO\")\n",
    "        print(f\"   ‚Ä¢ Filtros: NOME_EOL, UF, faixas de pot√™ncia\")\n",
    "        \n",
    "        print(f\"\\nüìä CAMPOS PRINCIPAIS:\")\n",
    "        print(f\"   ‚Ä¢ latitude/longitude - Para mapas\")\n",
    "        print(f\"   ‚Ä¢ POT_MW - Pot√™ncia em MW\")\n",
    "        print(f\"   ‚Ä¢ ALT_TOTAL/ALT_TORRE - Alturas\")\n",
    "        print(f\"   ‚Ä¢ DIAM_ROTOR - Di√¢metro do rotor\")\n",
    "        print(f\"   ‚Ä¢ NOME_EOL - Nome do parque e√≥lico\")\n",
    "        print(f\"   ‚Ä¢ DATA_ATUALIZACAO - Data da √∫ltima atualiza√ß√£o\")\n",
    "        \n",
    "        print(f\"\\nüéØ OBJETIVO DO CASE:\")\n",
    "        print(f\"   ‚Ä¢ Criar 1 dashboard no Tableau Public\")\n",
    "        print(f\"   ‚Ä¢ M√°ximo 5 slides no Google Slides com insights\")\n",
    "        print(f\"   ‚Ä¢ Evidenciar insights com prints do dashboard\")\n",
    "else:\n",
    "    print(f\"‚ùå Nenhum CSV dispon√≠vel para Tableau\")\n",
    "    print(f\"Execute o pipeline completo primeiro.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"üéâ PIPELINE HOMETOWN CONCLU√çDO!\")\n",
    "print(f\"üí° Use este notebook como refer√™ncia para futuras execu√ß√µes.\")\n",
    "print(f\"üîÑ O sistema √© idempotente - pode executar quantas vezes quiser.\")\n",
    "print(f\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
