{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hometown - Analytics Case\n",
    "## Pipeline Principal - ExtraÃ§Ã£o de Dados SIGEL/ANEEL\n",
    "\n",
    "Este notebook contÃ©m o pipeline principal para extraÃ§Ã£o de dados de aerogeradores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas\n",
    "\n",
    "# Adicionar src ao path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Imports do projeto\n",
    "from extraction.extractors import SigelExtractor\n",
    "from extraction.validators import (\n",
    "    validate_api_connection, \n",
    "    validate_response_structure,\n",
    "    validate_extraction_results\n",
    ")\n",
    "from config.settings import SIGEL_CONFIG\n",
    "from utils.logger import setup_logger\n",
    "from utils.exceptions import APIConnectionError, ValidationError\n",
    "\n",
    "# Setup logger\n",
    "logger = setup_logger(__name__, \"extraction.log\")\n",
    "\n",
    "print(\"Setup concluÃ­do!\")\n",
    "print(f\"Projeto root: {project_root}\")\n",
    "print(f\"URL da API: {SIGEL_CONFIG['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ValidaÃ§Ã£o da Conectividade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar conexÃ£o com a API\n",
    "try:\n",
    "    logger.info(\"Iniciando validaÃ§Ã£o de conectividade...\")\n",
    "    \n",
    "    api_url = SIGEL_CONFIG[\"url\"]\n",
    "    connection_ok = validate_api_connection(api_url)\n",
    "    \n",
    "    if connection_ok:\n",
    "        print(\"âœ… API conectada com sucesso!\")\n",
    "        logger.info(\"ValidaÃ§Ã£o de conectividade concluÃ­da com sucesso\")\n",
    "    else:\n",
    "        print(\"âŒ Falha na conexÃ£o com API\")\n",
    "        \n",
    "except (APIConnectionError, ValidationError) as e:\n",
    "    print(f\"âŒ Erro de validaÃ§Ã£o: {e}\")\n",
    "    logger.error(f\"Erro de validaÃ§Ã£o: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro inesperado: {e}\")\n",
    "    logger.error(f\"Erro inesperado durante validaÃ§Ã£o: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ExtraÃ§Ã£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar extraÃ§Ã£o completa\n",
    "try:\n",
    "    logger.info(\"Iniciando extraÃ§Ã£o de dados...\")\n",
    "    print(\"ðŸš€ Iniciando extraÃ§Ã£o de dados dos aerogeradores...\")\n",
    "    \n",
    "    # Instanciar extrator\n",
    "    extractor = SigelExtractor()\n",
    "    \n",
    "    # Executar extraÃ§Ã£o\n",
    "    saved_files = extractor.extract_all_data()\n",
    "    \n",
    "    print(f\"\\nâœ… ExtraÃ§Ã£o concluÃ­da!\")\n",
    "    print(f\"ðŸ“ Arquivos salvos: {len(saved_files)}\")\n",
    "    \n",
    "    # Mostrar alguns arquivos salvos\n",
    "    print(\"\\nðŸ“‹ Primeiros arquivos salvos:\")\n",
    "    for i, file_path in enumerate(saved_files[:5]):\n",
    "        print(f\"  {i+1}. {Path(file_path).name}\")\n",
    "    \n",
    "    if len(saved_files) > 5:\n",
    "        print(f\"  ... e mais {len(saved_files) - 5} arquivos\")\n",
    "        \n",
    "    logger.info(f\"ExtraÃ§Ã£o concluÃ­da com sucesso. {len(saved_files)} arquivos salvos\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro durante extraÃ§Ã£o: {e}\")\n",
    "    logger.error(f\"Erro durante extraÃ§Ã£o: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ValidaÃ§Ã£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar resultados da extraÃ§Ã£o\n",
    "try:\n",
    "    if 'saved_files' in locals() and saved_files:\n",
    "        logger.info(\"Iniciando validaÃ§Ã£o dos resultados...\")\n",
    "        print(\"ðŸ” Validando resultados da extraÃ§Ã£o...\")\n",
    "        \n",
    "        # ValidaÃ§Ã£o bÃ¡sica (expandir posteriormente)\n",
    "        validation_ok = validate_extraction_results(\n",
    "            saved_files=saved_files,\n",
    "            expected_records=0  # TODO: usar contagem real\n",
    "        )\n",
    "        \n",
    "        if validation_ok:\n",
    "            print(\"âœ… ValidaÃ§Ã£o dos resultados concluÃ­da!\")\n",
    "            logger.info(\"ValidaÃ§Ã£o dos resultados concluÃ­da com sucesso\")\n",
    "        else:\n",
    "            print(\"âŒ Falha na validaÃ§Ã£o dos resultados\")\n",
    "            \n",
    "    else:\n",
    "        print(\"âš ï¸ Nenhum arquivo para validar. Execute a extraÃ§Ã£o primeiro.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro durante validaÃ§Ã£o: {e}\")\n",
    "    logger.error(f\"Erro durante validaÃ§Ã£o dos resultados: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resumo da ExecuÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resumo final\n",
    "if 'saved_files' in locals():\n",
    "    print(\"ðŸ“Š RESUMO DA EXTRAÃ‡ÃƒO\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"ðŸ—‚ï¸  Total de arquivos: {len(saved_files)}\")\n",
    "    \n",
    "    # Calcular tamanho total dos arquivos\n",
    "    total_size = 0\n",
    "    for file_path in saved_files:\n",
    "        if Path(file_path).exists():\n",
    "            total_size += Path(file_path).stat().st_size\n",
    "    \n",
    "    print(f\"ðŸ’¾ Tamanho total: {total_size / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"ðŸ“ LocalizaÃ§Ã£o: data/raw/\")\n",
    "    print(f\"ðŸ•’ PadrÃ£o de nomes: aerogeradores_raw_YYYYMMDD_HHMMSS_page_XXXX.json\")\n",
    "    \n",
    "    # Status final\n",
    "    print(\"\\nðŸŽ‰ ExtraÃ§Ã£o de dados concluÃ­da com sucesso!\")\n",
    "    print(\"ðŸ”„ PrÃ³ximo passo: TransformaÃ§Ã£o dos dados (JSON â†’ Parquet)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Nenhuma extraÃ§Ã£o foi executada nesta sessÃ£o.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Debug e ExploraÃ§Ã£o (Opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar estrutura de um arquivo extraÃ­do\n",
    "import json\n",
    "\n",
    "if 'saved_files' in locals() and saved_files:\n",
    "    # Ler primeiro arquivo como exemplo\n",
    "    first_file = saved_files[0]\n",
    "    print(f\"ðŸ” Explorando estrutura do arquivo: {Path(first_file).name}\")\n",
    "    \n",
    "    with open(first_file, 'r', encoding='utf-8') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Estrutura do JSON:\")\n",
    "    print(f\"  - Chaves principais: {list(sample_data.keys())}\")\n",
    "    \n",
    "    if 'features' in sample_data:\n",
    "        features = sample_data['features']\n",
    "        print(f\"  - Total de features: {len(features)}\")\n",
    "        \n",
    "        if features:\n",
    "            first_feature = features[0]\n",
    "            print(f\"  - Estrutura da feature:\")\n",
    "            print(f\"    - Chaves: {list(first_feature.keys())}\")\n",
    "            \n",
    "            if 'attributes' in first_feature:\n",
    "                attrs = first_feature['attributes']\n",
    "                print(f\"    - Attributes: {list(attrs.keys())[:10]}...\")  # Primeiros 10\n",
    "            \n",
    "            if 'geometry' in first_feature:\n",
    "                geom = first_feature['geometry']\n",
    "                print(f\"    - Geometry type: {geom.get('type', 'N/A')}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Nenhum arquivo disponÃ­vel para exploraÃ§Ã£o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de import\n",
    "try:\n",
    "    from transformation.processors import DataProcessor\n",
    "    from transformation.geo_utils import extract_coordinates, validate_geometry\n",
    "    print(\"âœ… Imports da transformaÃ§Ã£o OK!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro no import: {e}\")\n",
    "\n",
    "# Teste bÃ¡sico\n",
    "processor = DataProcessor()\n",
    "json_files = processor.discover_raw_files()\n",
    "print(f\"ðŸ“ Arquivos JSON encontrados: {len(json_files)}\")\n",
    "\n",
    "# Testar um arquivo sÃ³\n",
    "if json_files:\n",
    "    print(f\"ðŸ” Testando: {json_files[0].name}\")\n",
    "    test_result = processor.process_single_file(json_files[0])\n",
    "    print(f\"âœ… Resultado: {test_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar todos os 24 arquivos JSON â†’ Parquet\n",
    "print(\"ðŸš€ Processando todos os arquivos...\")\n",
    "all_results = processor.process_all_files()\n",
    "\n",
    "print(f\"\\nðŸ“Š RESULTADO FINAL:\")\n",
    "print(f\"âœ… Arquivos processados: {len(all_results)}\")\n",
    "\n",
    "# Verificar tamanho total\n",
    "total_size = 0\n",
    "for file_path in all_results:\n",
    "    if Path(file_path).exists():\n",
    "        total_size += Path(file_path).stat().st_size\n",
    "\n",
    "print(f\"ðŸ’¾ Tamanho total Parquets: {total_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"ðŸ—‚ï¸ LocalizaÃ§Ã£o: data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de import\n",
    "try:\n",
    "    from consolidation.consolidators import DataConsolidator\n",
    "    print(\"âœ… Import da consolidaÃ§Ã£o OK!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro no import: {e}\")\n",
    "\n",
    "# Instanciar consolidador\n",
    "consolidator = DataConsolidator()\n",
    "\n",
    "# Descobrir arquivos Parquet\n",
    "parquet_files = consolidator.discover_parquet_files()\n",
    "print(f\"ðŸ“ Arquivos Parquet encontrados: {len(parquet_files)}\")\n",
    "\n",
    "# Executar consolidaÃ§Ã£o completa\n",
    "print(\"ðŸš€ Iniciando consolidaÃ§Ã£o completa...\")\n",
    "output_path = consolidator.consolidate_all()\n",
    "\n",
    "print(f\"\\nâœ… ConsolidaÃ§Ã£o concluÃ­da!\")\n",
    "print(f\"ðŸ“„ Arquivo CSV: {Path(output_path).name}\")\n",
    "print(f\"ðŸ“ LocalizaÃ§Ã£o: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar import do pandas\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Carregar o CSV para anÃ¡lise\n",
    "df_final = pd.read_csv(output_path)\n",
    "\n",
    "# Gerar resumo\n",
    "summary = consolidator.get_data_summary(df_final)\n",
    "\n",
    "print(\"\\nðŸ“Š RESUMO DOS DADOS CONSOLIDADOS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ðŸ“ Total de registros: {summary['total_records']:,}\")\n",
    "print(f\"ðŸ“‹ Total de colunas: {summary['total_columns']}\")\n",
    "print(f\"ðŸ’¾ Uso de memÃ³ria: {summary['memory_usage_mb']:.2f} MB\")\n",
    "\n",
    "if 'coordinate_stats' in summary:\n",
    "    coords = summary['coordinate_stats']\n",
    "    print(f\"\\nðŸ—ºï¸ COORDENADAS:\")\n",
    "    print(f\"  Latitude: {coords['lat_min']:.6f} â†’ {coords['lat_max']:.6f}\")\n",
    "    print(f\"  Longitude: {coords['lon_min']:.6f} â†’ {coords['lon_max']:.6f}\")\n",
    "\n",
    "print(f\"\\nðŸ“‚ Colunas disponÃ­veis:\")\n",
    "for i, col in enumerate(summary['columns'][:10], 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "if len(summary['columns']) > 10:\n",
    "    print(f\"  ... e mais {len(summary['columns']) - 10} colunas\")\n",
    "\n",
    "# Verificar tamanho do arquivo\n",
    "file_size = Path(output_path).stat().st_size / 1024 / 1024\n",
    "print(f\"\\nðŸ’¾ Tamanho do CSV: {file_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
